{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T07:29:31.789590Z","iopub.status.busy":"2024-06-18T07:29:31.789065Z","iopub.status.idle":"2024-06-18T07:29:36.874466Z","shell.execute_reply":"2024-06-18T07:29:36.873187Z","shell.execute_reply.started":"2024-06-18T07:29:31.789550Z"},"trusted":true},"outputs":[],"source":["import joblib\n","import os\n","import shutil\n","import random\n","import pandas as pd\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from torch.utils.data import Dataset, DataLoader\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!unzip /kaggle/input/ifood-2019-fgvc6/train_set.zip -d train_set\n","!unzip /kaggle/input/ifood-2019-fgvc6/val_set.zip -d test_set"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T07:35:40.918977Z","iopub.status.busy":"2024-06-18T07:35:40.918446Z","iopub.status.idle":"2024-06-18T07:35:41.060256Z","shell.execute_reply":"2024-06-18T07:35:41.058765Z","shell.execute_reply.started":"2024-06-18T07:35:40.918939Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'# Limit images per class for train and test sets\\nlimit_images_per_class(train_labels, train_source_dir, limited_train_dir)\\nlimit_images_per_class(test_labels, test_source_dir, limited_test_dir)\\n# Now train data has 100 images per class, validation has different number of images per class max 100'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["from torchvision import transforms\n","from PIL import Image\n","import os\n","\n","# Function to limit images per class\n","def limit_images_per_class(labels_df, source_dir, dest_dir, limit=200):\n","    if not os.path.exists(dest_dir):\n","        os.makedirs(dest_dir)\n","    \n","    class_groups = labels_df.groupby('label')\n","    \n","    for label, group in class_groups:\n","        dest_class_dir = os.path.join(dest_dir, str(label))\n","        \n","        if not os.path.exists(dest_class_dir):\n","            os.makedirs(dest_class_dir)\n","        \n","        images = group['img_name'].tolist()\n","        selected_images = random.sample(images, min(limit, len(images)))\n","        \n","        for image_name in selected_images:\n","            source_image_path = os.path.join(source_dir, image_name)\n","            dest_image_path = os.path.join(dest_class_dir, image_name)\n","            shutil.copy(source_image_path, dest_image_path)\n","            \n","            \n","# Load the CSV files\n","train_labels = pd.read_csv('/kaggle/input/csv-files/train_labels.csv')\n","test_labels = pd.read_csv('/kaggle/input/csv-files/val_labels.csv')\n","# Define source and destination directories\n","train_source_dir = '/kaggle/working/train_set/train_set'\n","test_source_dir = '/kaggle/working/test_set/val_set'\n","limited_train_dir = '/kaggle/working/limited_train_set2'\n","limited_test_dir = '/kaggle/working/limited_test_set2'\n","\n","\"\"\"# Limit images per class for train and test sets\n","limit_images_per_class(train_labels, train_source_dir, limited_train_dir)\n","limit_images_per_class(test_labels, test_source_dir, limited_test_dir)\n","# Now train data has 100 images per class, validation has different number of images per class max 100\"\"\"\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T16:46:53.236217Z","iopub.status.busy":"2024-06-17T16:46:53.235877Z","iopub.status.idle":"2024-06-17T16:46:53.242307Z","shell.execute_reply":"2024-06-17T16:46:53.240988Z","shell.execute_reply.started":"2024-06-17T16:46:53.236188Z"},"trusted":true},"outputs":[],"source":["# Create train, test and validation data\n","# Directories\n","train_dir = '/kaggle/working/limited_train_set2'  # Your directory containing 251 folders for training\n","val_dir = '/kaggle/working/limited_val_set2'  # Directory to save validation data\n","test_dir = '/kaggle/working/limited_test_set2'  # Your directory containing 251 folders for testing\n","# Create validation directory\n","os.makedirs(val_dir)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-06-17T16:46:53.244523Z","iopub.status.busy":"2024-06-17T16:46:53.244161Z","iopub.status.idle":"2024-06-17T16:46:53.633234Z","shell.execute_reply":"2024-06-17T16:46:53.631811Z","shell.execute_reply.started":"2024-06-17T16:46:53.244494Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import random\n","from sklearn.model_selection import train_test_split\n","\n","# Function to split data into training and validation\n","def split_train_val(train_dir, val_dir, val_ratio=0.2):\n","    for class_name in os.listdir(train_dir):\n","        class_dir = os.path.join(train_dir, class_name)\n","        images = os.listdir(class_dir)\n","        \n","        train_images, val_images = train_test_split(images, test_size=val_ratio, random_state=42)\n","        \n","        val_class_dir = os.path.join(val_dir, class_name)\n","        if not os.path.exists(val_class_dir):\n","            os.makedirs(val_class_dir)\n","        \n","        for image in val_images:\n","            shutil.move(os.path.join(class_dir, image), os.path.join(val_class_dir, image))\n","# Perform the split\n","split_train_val(train_dir, val_dir)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T07:36:10.290546Z","iopub.status.busy":"2024-06-18T07:36:10.290073Z","iopub.status.idle":"2024-06-18T07:36:10.503943Z","shell.execute_reply":"2024-06-18T07:36:10.502375Z","shell.execute_reply.started":"2024-06-18T07:36:10.290514Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["func def ends\n"]}],"source":["import cv2\n","import numpy as np\n","import os\n","import random\n","import shutil\n","from sklearn.cluster import KMeans\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split\n","\n","# Function to limit images per class\n","def limit_images_per_class(data_dir, limit=200):\n","    limited_image_paths = []\n","    labels = []\n","    class_names = os.listdir(data_dir)\n","    \n","    for class_idx, class_name in enumerate(class_names):\n","        class_dir = os.path.join(data_dir, class_name)\n","        images = os.listdir(class_dir)\n","        selected_images = random.sample(images, min(limit, len(images)))\n","        \n","        for img_name in selected_images:\n","            img_path = os.path.join(class_dir, img_name)\n","            limited_image_paths.append(img_path)\n","            labels.append(class_idx)\n","    \n","    return limited_image_paths, labels, class_names\n","\n","# Function to extract SIFT features from images\n","def extract_sift_features(image_paths):\n","    sift = cv2.SIFT_create()\n","    descriptors_list = []\n","    for image_path in image_paths:\n","        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","        image = cv2.resize(image, (256, 256))  # Resize for consistency\n","        keypoints, descriptors = sift.detectAndCompute(image, None)\n","        if descriptors is not None:\n","            descriptors_list.append(descriptors)\n","    return descriptors_list\n","\n","# Create the Bag of Words model\n","def create_bow(descriptors_list, num_clusters):\n","    bow_trainer = cv2.BOWKMeansTrainer(num_clusters)\n","    for descriptors in descriptors_list:\n","        if descriptors is not None:\n","            bow_trainer.add(descriptors)\n","    vocabulary = bow_trainer.cluster()\n","    return vocabulary\n","\n","\n","# Create histograms of visual words\n","def create_histograms(image_paths, sift, bow_extractor):\n","    histograms = []\n","    for image_path in image_paths:\n","        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","        keypoints, descriptors = sift.detectAndCompute(image, None)\n","        if descriptors is not None:\n","            histogram = bow_extractor.compute(image, keypoints)\n","            histograms.append(histogram)\n","        else:\n","            histogram = np.zeros((1, bow_extractor.getVocabulary().shape[0]), dtype=np.float32)\n","            histograms.append(histogram)\n","    return np.array(histograms)\n","\n","\n","\n","print(\"func def ends\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","# Load and limit image paths and labels\n","train_image_paths, train_labels, class_names = limit_images_per_class(train_dir)\n","val_image_paths, val_labels, _ = limit_images_per_class(val_dir)\n","test_image_paths, test_labels, _ = limit_images_per_class(test_dir)\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:43:26.972592Z","iopub.status.busy":"2024-06-16T15:43:26.971885Z","iopub.status.idle":"2024-06-16T15:43:26.981445Z","shell.execute_reply":"2024-06-16T15:43:26.980568Z","shell.execute_reply.started":"2024-06-16T15:43:26.972558Z"},"trusted":true},"outputs":[],"source":["import joblib\n","import os\n","import numpy as np\n","\n","# Paths to save the BoW vocabulary and histograms\n","vocabulary_path = 'bow_vocabulary.pkl'\n","train_histograms_path = 'train_histograms.pkl'\n","val_histograms_path = 'val_histograms.pkl'\n","test_histograms_path = 'test_histograms.pkl'\n","\n","# Function to load or create and save the BoW vocabulary\n","def load_or_create_bow(descriptors_list, num_clusters, bow_path):\n","    if os.path.exists(bow_path):\n","        vocabulary = joblib.load(bow_path)\n","        print(f\"BoW vocabulary loaded from {bow_path}\")\n","    else:\n","        vocabulary = create_bow(descriptors_list, num_clusters)\n","        joblib.dump(vocabulary, bow_path)\n","        print(f\"BoW vocabulary saved to {bow_path}\")\n","    return vocabulary\n","\n","# Function to load or create and save histograms\n","def load_or_create_histograms(image_paths, sift, bow_extractor, histograms_path):\n","    if os.path.exists(histograms_path):\n","        histograms = joblib.load(histograms_path)\n","        print(f\"Histograms loaded from {histograms_path}\")\n","    else:\n","        histograms = create_histograms(image_paths, sift, bow_extractor)\n","        joblib.dump(histograms, histograms_path)\n","        print(f\"Histograms saved to {histograms_path}\")\n","    return histograms\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create or load BoW vocabulary\n","num_clusters = 100\n","vocabulary = load_or_create_bow(train_descriptors_list, num_clusters, vocabulary_path)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-06-16T15:43:54.183781Z","iopub.status.busy":"2024-06-16T15:43:54.182933Z","iopub.status.idle":"2024-06-16T16:24:02.787009Z","shell.execute_reply":"2024-06-16T16:24:02.786130Z","shell.execute_reply.started":"2024-06-16T15:43:54.183748Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Histograms saved to train_histograms.pkl\n","Histograms saved to val_histograms.pkl\n","Histograms saved to test_histograms.pkl\n"]}],"source":["\"\"\"\n","# Create or load histograms of visual words\n","train_histograms = load_or_create_histograms(train_descriptors_list, vocabulary, train_histograms_path)\n","val_histograms = load_or_create_histograms(val_descriptors_list, vocabulary, val_histograms_path)\n","test_histograms = load_or_create_histograms(test_descriptors_list, vocabulary, test_histograms_path) \"\"\"\n","\n","# Create BoW extractor\n","sift = cv2.SIFT_create()\n","flann_params = dict(algorithm=1, trees=5)\n","flann_matcher = cv2.FlannBasedMatcher(flann_params, {})\n","bow_extractor = cv2.BOWImgDescriptorExtractor(sift, flann_matcher)\n","bow_extractor.setVocabulary(vocabulary)\n","\n","# Create or load histograms of visual words\n","train_histograms = load_or_create_histograms(train_image_paths, sift, bow_extractor, train_histograms_path)\n","val_histograms = load_or_create_histograms(val_image_paths, sift, bow_extractor, val_histograms_path)\n","test_histograms = load_or_create_histograms(test_image_paths, sift, bow_extractor, test_histograms_path)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T07:37:30.416740Z","iopub.status.busy":"2024-06-18T07:37:30.416254Z","iopub.status.idle":"2024-06-18T07:37:30.557917Z","shell.execute_reply":"2024-06-18T07:37:30.556475Z","shell.execute_reply.started":"2024-06-18T07:37:30.416703Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Histograms loaded from /kaggle/input/sift-bow-hisogram/train_histograms.pkl\n","Histograms loaded from /kaggle/input/sift-bow-hisogram/val_histograms.pkl\n","Histograms loaded from /kaggle/input/sift-bow-hisogram/test_histograms.pkl\n"]}],"source":["# Function to load histograms\n","def load_histograms(histograms_path):\n","    if os.path.exists(histograms_path):\n","        histograms = joblib.load(histograms_path)\n","        print(f\"Histograms loaded from {histograms_path}\")\n","    else:\n","        raise FileNotFoundError(f\"Histograms file {histograms_path} not found.\")\n","    return histograms\n","\n","train_histograms_path = \"/kaggle/input/sift-bow-hisogram/train_histograms.pkl\"\n","val_histograms_path = \"/kaggle/input/sift-bow-hisogram/val_histograms.pkl\"\n","test_histograms_path = \"/kaggle/input/sift-bow-hisogram/test_histograms.pkl\"\n","\n","# Load histograms for train, validation, and test datasets\n","train_histograms = load_histograms(train_histograms_path)\n","val_histograms = load_histograms(val_histograms_path)\n","test_histograms = load_histograms(test_histograms_path)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-06-18T07:37:37.890168Z","iopub.status.busy":"2024-06-18T07:37:37.889692Z","iopub.status.idle":"2024-06-18T07:37:37.897552Z","shell.execute_reply":"2024-06-18T07:37:37.896117Z","shell.execute_reply.started":"2024-06-18T07:37:37.890135Z"},"trusted":true},"outputs":[],"source":["# Ensure histograms are in the correct shape\n","train_histograms = train_histograms.reshape(len(train_histograms), -1)\n","val_histograms = val_histograms.reshape(len(val_histograms), -1)\n","test_histograms = test_histograms.reshape(len(test_histograms), -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","\n","# Define the parameter grid\n","param_grid = {\n","    'C': [1, 10, 100],\n","    'gamma': ['scale', 'auto', 0.01],\n","    'kernel': ['rbf', 'sigmoid']\n","}\n","\n","# Perform grid search with cross-validation\n","grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n","grid_search.fit(train_histograms, train_labels)\n","\n","# Output the best parameters and best score\n","print(f\"Best parameters: {grid_search.best_params_}\")\n","print(f\"Best cross-validation accuracy: {grid_search.best_score_}\")\n","\n","# Train the final model with the best parameters\n","best_svm = grid_search.best_estimator_\n","best_svm.fit(train_histograms, train_labels)\n","\n","# Evaluate the model on validation and test sets\n","val_predictions = best_svm.predict(val_histograms)\n","test_predictions = best_svm.predict(test_histograms)\n","\n","val_accuracy = accuracy_score(val_labels, val_predictions)\n","test_accuracy = accuracy_score(test_labels, test_predictions)\n","\n","val_accuracy = accuracy_score(val_labels, val_predictions)\n","val_precision = precision_score(val_labels, val_predictions, average='weighted')\n","val_recall = recall_score(val_labels, val_predictions, average='weighted')\n","val_f1 = f1_score(val_labels, val_predictions, average='weighted')\n","\n","test_accuracy = accuracy_score(test_labels, test_predictions)\n","test_precision = precision_score(test_labels, test_predictions, average='weighted')\n","test_recall = recall_score(test_labels, test_predictions, average='weighted')\n","test_f1 = f1_score(test_labels, test_predictions, average='weighted')\n","\n","print(f\"Validation Accuracy: {val_accuracy}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n","\n","print(f\"Validation Accuracy: {val_accuracy}\")\n","print(f\"Validation Precision: {val_precision}\")\n","print(f\"Validation Recall: {val_recall}\")\n","print(f\"Validation F1-score: {val_f1}\")\n","\n","print(f\"Test Accuracy: {test_accuracy}\")\n","print(f\"Test Precision: {test_precision}\")\n","print(f\"Test Recall: {test_recall}\")\n","print(f\"Test F1-score: {test_f1}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Train the final model with the best parameters\n","# Combine train and validation data\n","combined_histograms = np.vstack((train_histograms, val_histograms))\n","combined_labels = np.hstack((train_labels, val_labels))\n","\n","# Scale the combined histograms\n","scaler = StandardScaler().fit(combined_histograms)\n","combined_histograms = scaler.transform(combined_histograms)\n","test_histograms = scaler.transform(test_histograms)\n","\n","best_svm = SVC(C=1.0, gamma='scale', kernel='rbf') #enter de final(best) params \n","best_svm.fit(train_histograms, train_labels)\n","\n","# Train the SVM model with cross-validation to plot the performance\n","cv = StratifiedKFold(n_splits=5)\n","val_losses = []\n","\n","for train_index, val_index in cv.split(combined_histograms, combined_labels):\n","    X_train, X_val = combined_histograms[train_index], combined_histograms[val_index]\n","    y_train, y_val = combined_labels[train_index], combined_labels[val_index]\n","    \n","    best_svm.fit(X_train, y_train)\n","    val_score = best_svm.score(X_val, y_val)\n","    val_loss = 1 - val_score  # Loss is 1 - accuracy\n","    val_losses.append(val_loss)\n","\n","# Plot the validation loss\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, len(val_losses) + 1), val_losses, marker='o', linestyle='-', color='b')\n","plt.title('Validation Loss Across Cross-Validation Folds')\n","plt.xlabel('Fold')\n","plt.ylabel('Validation Loss')\n","plt.ylim([0, 1])\n","plt.grid()\n","plt.show()\n","\n","\n","# Train the best model on the combined training and validation data\n","best_svm.fit(combined_histograms, combined_labels)\n","\n","# Save the final model\n","joblib.dump(best_svm, 'best_svm_model_final.pkl')\n","print(f\"Final model saved as 'best_svm_model_final.pkl'\")\n","\n","# Save the final model\n","joblib.dump(best_svm, 'best_svm_model.pkl')\n","print(f\"Final model saved as 'best_svm_model.pkl'\")\n","\n","\n","# Evaluate the final model on the test set\n","test_accuracy = best_svm.score(test_histograms, test_labels)\n","print(f'Test Accuracy: {test_accuracy}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":401923,"sourceId":13663,"sourceType":"competition"},{"datasetId":5228136,"sourceId":8714349,"sourceType":"datasetVersion"},{"datasetId":5231026,"sourceId":8718339,"sourceType":"datasetVersion"},{"datasetId":5231058,"sourceId":8718379,"sourceType":"datasetVersion"}],"dockerImageVersionId":30733,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
